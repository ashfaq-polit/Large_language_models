{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyM47R3hQrxDsiIjvEVpOS63",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashfaq-polit/Large_language_models/blob/master/Reranking_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-run setup after Colab reset\n",
        "!pip install -qU \\\n",
        "    datasets==2.14.5 \\\n",
        "    faiss-cpu \\\n",
        "    sentence-transformers \\\n",
        "    langchain \\\n",
        "    langchain-community\n",
        "\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "from tqdm.auto import tqdm\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "import torch\n",
        "\n",
        "# Load dataset\n",
        "data = load_dataset(\"jamescalam/ai-arxiv-chunked\", split=\"train\")\n",
        "\n",
        "# Preprocess dataset\n",
        "data = data.map(lambda x: {\n",
        "    \"id\": f'{x[\"id\"]}-{x[\"chunk-id\"]}',\n",
        "    \"text\": x[\"chunk\"],\n",
        "    \"metadata\": {\n",
        "        \"title\": x[\"title\"],\n",
        "        \"url\": x[\"source\"],\n",
        "        \"primary_category\": x[\"primary_category\"],\n",
        "        \"published\": x[\"published\"],\n",
        "        \"updated\": x[\"updated\"],\n",
        "        \"text\": x[\"chunk\"],\n",
        "    }\n",
        "})\n",
        "data = data.remove_columns([\n",
        "    \"title\", \"summary\", \"source\", \"authors\", \"categories\", \"comment\",\n",
        "    \"journal_ref\", \"primary_category\", \"published\", \"updated\", \"references\",\n",
        "    \"doi\", \"chunk-id\", \"chunk\"\n",
        "])\n",
        "\n",
        "# Initialize embedding model (no API key needed)\n",
        "embed_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"}\n",
        ")\n",
        "\n",
        "# Create FAISS index\n",
        "index_path = \"rerankers_faiss_index\"\n",
        "if os.path.exists(index_path):\n",
        "    print(\"📂 Loading existing FAISS index...\")\n",
        "    index = FAISS.load_local(index_path, embed_model, allow_dangerous_deserialization=True)\n",
        "else:\n",
        "    print(\"🆕 Creating new FAISS index...\")\n",
        "    documents = [\n",
        "        Document(page_content=data[i][\"text\"], metadata=data[i][\"metadata\"])\n",
        "        for i in range(min(len(data), 500))  # limit for demo\n",
        "    ]\n",
        "    index = FAISS.from_documents(documents, embed_model)\n",
        "    index.save_local(index_path)\n",
        "\n",
        "# Function to retrieve similar documents\n",
        "def get_docs(query: str, top_k: int) -> list[str]:\n",
        "    results = index.similarity_search(query, k=top_k)\n",
        "    return [doc.page_content for doc in results]\n",
        "\n",
        "# Query example\n",
        "query = \"can you explain why we would want to do rlhf?\"\n",
        "docs = get_docs(query, top_k=10)\n",
        "docs[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmf0ZWA_z0Dw",
        "outputId": "62d72ec5-8a31-481a-e211-085f10a76dac"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📂 Loading existing FAISS index...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Human I really need the books and at least one other object.\\nRL+ROLLOUTS Ok, you can have one book and one ball\\nHuman If I can have all the books, I can leave you the rest.\\nRL+ROLLOUTS Deal\\nOutput Reward\\nRL+ROLLOUTS 3xhat1xball 7/10\\nHuman 3x book 6/10\\nFigure 6: Example of model compromising.\\nutterances produced by our model, and found that\\nthe overwhelming majority were ﬂuent English\\nsentences in isolation—showing that the model\\nhas learnt a good language model for the domain\\n(in addition to results that show it uses language\\neffectively to achieve its goals). These results suggest that although neural models are prone to the\\nsafer option of repeating sentences from training\\ndata, they are capable of generalising when necessary. Future work should choose domains that\\nforce a higher degree of diversity in utterances.\\nMaintaining multi-sentence coherence is challenging. One common linguistic error we see\\nRL+ROLLOUTS make is to start a message by indicating agreement (e.g. I agree orDeal ), but then\\ngoing on to propose a counter offer—a behaviour\\nthat human partners found frustrating. One explanation is that the model has learnt that in the\\nsupervised data, messages beginning with I agree\\nare often at the end of the dialogue, and partners',\n",
              " 'Similar trends hold in dialogues with humans,\\nwith goal-based reasoning outperforming imitation learning. The ROLLOUTS model achieves\\ncomparable scores to its human partners, and the\\nRL+ROLLOUTS model actually achieves higher\\nscores. However, we also ﬁnd signiﬁcantly more\\ncases of the goal-based models failing to agree a\\ndeal with humans—largely a consequence of their\\nmore aggressive negotiation tactics (see §7).7 Analysis\\nTable 1 shows large gains from goal-based methods. In this section, we explore the strengths and\\nweaknesses of our models.\\nGoal-based models negotiate harder. The\\nRL+ROLLOUTS model has much longer dialogues\\nwith humans than LIKELIHOOD (7.2 turns vs. 5.3\\non average), indicating that the model is accepting\\ndeals less quickly, and negotiating harder.\\nA negative consequence of this more aggressive negotiation strategy is that humans were more\\nlikely to walk away with no deal, which is reﬂected in the lower agreement rates. Even though\\nfailing to agree was worth 0 points, people often\\npreferred this course over capitulating to an uncompromising opponent—a factor not well captured by the simulated partner in reinforcement\\nlearning training or rollouts (as reﬂected by the\\nlarger gains from goal-based models in dialogues',\n",
              " 'Model Valid PPL Test PPL Test Avg. Rank\\nLIKELIHOOD 5.62 5.47 521.8\\nRL 6.03 5.86 517.6\\nROLLOUTS - - 844.1\\nRL+ROLLOUTS - - 859.8\\nTable 3: Intrinsic evaluation showing the average\\nperplexity of tokens and rank of complete turns\\n(out of 2083 unique human messages from the test\\nset). Lower is more human-like for both.\\nResults are shown in Table 1. Firstly,\\nwe see that the RLand ROLLOUTS models\\nachieve signiﬁcantly better results when negotiating with the LIKELIHOOD model, particularly the\\nRL+ROLLOUTS model. The percentage of Pareto\\noptimal solutions also increases, showing a better exploration of the solution space. Compared\\nto human-human negotiations (Table 2), the best\\nmodels achieve a higher agreement rate, better\\nscores, and similar Pareto efﬁciency. This result\\nconﬁrms that attempting to maximise reward can\\noutperform simply imitating humans.\\nSimilar trends hold in dialogues with humans,\\nwith goal-based reasoning outperforming imitation learning. The ROLLOUTS model achieves\\ncomparable scores to its human partners, and the']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GU0CX07bz_uO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *ReRanking*"
      ],
      "metadata": {
        "id": "xVXkKoTJ06z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import numpy as np\n",
        "from typing import List, Dict\n",
        "\n",
        "# 1. Load local embedding model\n",
        "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")  # no API key required\n",
        "\n",
        "# 2. Define rerank function\n",
        "def rerank(query: str, documents: List[str], top_n: int = 3) -> List[Dict]:\n",
        "    # Embed query and documents\n",
        "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "    doc_embeddings = model.encode(documents, convert_to_tensor=True)\n",
        "\n",
        "    # Compute cosine similarities\n",
        "    scores = util.pytorch_cos_sim(query_embedding, doc_embeddings)[0]\n",
        "\n",
        "    # Get top_n indices sorted by similarity\n",
        "    top_results = np.argsort(-scores.cpu().numpy())[:top_n]\n",
        "\n",
        "    # Return reranked docs and scores\n",
        "    reranked = [{\"index\": i, \"score\": scores[i].item(), \"text\": documents[i]} for i in top_results]\n",
        "    return reranked\n"
      ],
      "metadata": {
        "id": "zIXAYcU209MA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare(query: str, top_k: int, top_n: int):\n",
        "    # Get vector search results (assumed to be a list of strings)\n",
        "    docs = get_docs(query, top_k=top_k)\n",
        "    doc_list = docs  # already a list of text\n",
        "    i2doc = {i: doc for i, doc in enumerate(doc_list)}  # optional for debugging\n",
        "\n",
        "    # Rerank\n",
        "    reranked = rerank(query, doc_list, top_n)\n",
        "\n",
        "\n",
        "    # Compare order\n",
        "    print(f\"\\nQuery: {query}\\n\")\n",
        "    for i, result in enumerate(reranked):\n",
        "        orig_pos = doc_list.index(result[\"text\"])\n",
        "        print(f\"[{i}] Reranked from original index {orig_pos} with score {result['score']:.4f}\")\n",
        "        print(result[\"text\"])\n",
        "        print(\"\\n---\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "1qaJ0P711Fxe"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare(\"what is red teaming?\", top_k=25, top_n=3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HZlMc571UMT",
        "outputId": "75030e2c-417c-476f-ff8b-d47f6d11810a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: what is red teaming?\n",
            "\n",
            "[0] Reranked from original index 0 with score 0.2547\n",
            "(q,a)pairs as output. Checkpoints are selected\n",
            "by Exact Match score on a development set. We\n",
            "also include a much more powerful T5-11B model\n",
            "from Roberts et al. (2020 ). We use the T511B model which has been pretrained with a special “Salient Span Masking” objective ( Guu et al. ,\n",
            "2020 ), designed to improve downstream ODQA\n",
            "ModelOpen Natural Questions TriviaQA WebQuestions\n",
            "TotalQuestion\n",
            "OverlapAnswer\n",
            "Overlap\n",
            "OnlyNo\n",
            "OverlapTotalQuestion\n",
            "OverlapAnswer\n",
            "Overlap\n",
            "OnlyNo\n",
            "OverlapTotalQuestion\n",
            "OverlapAnswer\n",
            "Overlap\n",
            "OnlyNo\n",
            "Overlap\n",
            "Open\n",
            "bookRAG 44.5 70.7 34.9 24.8 56.8 82.7 54.7 29.2 45.5 81.0 45.8 21.1\n",
            "DPR 41.3 69.4 34.6 19.3 57.9 80.4 59.6 31.6 42.4 74.1 39.8 22.2\n",
            "FID 51.4 71.3 48.3 34.5 67.6 87.5 66.9 42.8 - - - Closed\n",
            "bookT5-11B+SSM 36.6 77.2 22.2 9.4 - - - - 44.7 82.1 44.5 22.0\n",
            "\n",
            "---\n",
            "\n",
            "[1] Reranked from original index 1 with score 0.2372\n",
            "Centipede 3496.5 8904.8 4386.4\n",
            "ChopperCommand 1171.7 5287.7 3516.3\n",
            "CrazyClimber 107770.0 132461.0 110202.0\n",
            "DemonAttack 6639.1 38808.3 11378.4\n",
            "DoubleDunk -16.2 -13.2 -14.9\n",
            "Enduro 0.0 0.0 758.3\n",
            "FishingDerby 20.6 34.7 17.8\n",
            "Freeway 0.0 0.0 32.5\n",
            "Frostbite 261.8 285.6 314.2\n",
            "Gopher 1500.9 37802.3 2932.9\n",
            "Gravitar 194.0 225.3 737.2\n",
            "IceHockey -6.4 -5.9 -4.2\n",
            "Jamesbond 52.3 261.8 560.7\n",
            "Kangaroo 45.3 50.0 9928.7\n",
            "Krull 8367.4 7268.4 7942.3\n",
            "KungFuMaster 24900.3 27599.3 23310.3\n",
            "MontezumaRevenge 0.0 0.3 42.0\n",
            "MsPacman 1626.9 2718.5 2096.5\n",
            "NameThisGame 5961.2 8488.0 6254.9\n",
            "\n",
            "---\n",
            "\n",
            "[2] Reranked from original index 2 with score 0.2144\n",
            "We also studied whether we could add another step of distillation during the adaptation phase by\n",
            "ﬁne-tuning DistilBERT on SQuAD using a BERT model previously ﬁne-tuned on SQuAD as a\n",
            "4We use jiant [Wang et al., 2019] to compute the baseline.\n",
            "3\n",
            "Table 4: Ablation study. Variations are relative to the model trained with triple loss and teacher\n",
            "weights initialization.\n",
            "Ablation Variation on GLUE macro-score\n",
            ";-Lcos-Lmlm -2.96\n",
            "Lce-;-Lmlm -1.46\n",
            "Lce-Lcos-; -0.31\n",
            "Triple loss + random weights initialization -3.69\n",
            "teacher for an additional term in the loss (knowledge distillation). In this setting, there are thus two\n",
            "successive steps of distillation, one during the pre-training phase and one during the adaptation phase.\n",
            "In this case, we were able to reach interesting performances given the size of the model: 79.8 F1 and\n",
            "70.4 EM, i.e. within 3 points of the full model.\n",
            "Size and inference speed\n",
            "To further investigate the speed-up/size trade-off of DistilBERT, we compare (in Table 3) the number\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ]
    }
  ]
}